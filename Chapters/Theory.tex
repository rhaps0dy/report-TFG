\chapter{Background}
The immediate aim of this thesis is to produce a computer program that plays
\acl{MR} well. This problem statement suffices for most communication purposes,
but does not give us enough understanding to reason about the problem and find
ways to solve it. We first need to develop a formal definition of all the
notions: ``to play'', ``\acl{MR}'' and ``well''. We also need ways to know what
to do to play well. Fortunately, most of the required work has already been
done, by other authors.

In this chapter we will define mathematical models for the problem we are
facing. We will also formally define the algorithms we will use to tackle it,
without concerning ourselves with the details of their implementation on our
computing environment.

\section{\aclp{SDP}}
In this section, we describe the \acf{SDP} and related models. Most of the
definitions are taken from the \ac{RL} reference textbook by
\citet{sutton1998introduction}. Some are from the \ac{AI} reference textbook by
\citet{russell2009aima}. Concrete citations will be given after some claims, but
otherwise assume the concepts are taken from the first book.

Let us describe the \ac{SDP} model from
\citet[Section~3.1]{sutton1998introduction}. There is an \newconcept{agent} that, every
\newconcept{time step}, takes an \newconcept{action} in the \newconcept{environment}. The
environment is a process that has a \newconcept{state}. When the agent takes an
action, the environment's state changes. The agent also receives a \newconcept{reward}
when it takes an action.

More formally. There is a series of discrete time steps, $t=0,1,2,3,\dots$, in
which the agent and the environment interact. At time step $t$, environment is
in state $s_t \in \mathcal{S}$. $\mathcal{S}$ is the finite, but usually very
large, set of possible states. The agent takes an action $a_t$ from the set of
possible actions in the state, $a_t \in \mathcal{A}_{s_t}$. In the next time
step, the agent receives a numerical reward $r_{t+1} \in \mathfrak{R}$, and the
environment transitions to a new state $s_{t+1} \in \mathcal{S}$.

The environment defines the set of possible states $\mathcal{S}$, the possible
actions in each state, which belong to the set of all possible actions
$\mathcal{A}_s \subseteq \mathcal{A}$, the reward for each state $\mathcal{S}$
and the rules for transitioning to the new state in each time step. The agent
simply chooses the action $a_t$ in each time step. The interaction between agent
and environment is illustrated in \ffref{fig:agent-environment}.

\inkscapefig{agent-environment}{Diagram of interaction between agent and
  environment \citep[Section~3.1]{sutton1998introduction}}

This model is really flexible. It does not constrain time steps to have the same
length, so each can represent a decision branching point and not actual time. An
example of this can be observed in go, chess, poker and others, where each move
takes a different amount of wall clock time. It is also not constrained how a new
state is chosen after an action: it may depend on anything, maybe even be
stochastic and not deterministic.

The model also accepts different abstraction levels of states and actions: in a
video game, they can be raw pixel data and controller input, or entity position
representation and moving to a certain screen. This idea is the basis of
hierarchical reinforcement learning, which is explained in
\ffref{sec:hierarchical-rl}.

It is important to understand that the agent is only the \emph{process} that
\emph{decides} actions, not a physical object or entity. In the case of a robot, the
agent is only the controlling program: the actuators, mechanisms and sensors are part
of the environment. In the case of a video game, the code that emulates
the world and accepts controls is the environment, and the code that trains a
model or plans actions is the agent. This is the case even if the actions to be
taken are high-level, and not settings of force or torque on actuators or
muscles.

Observe also that the reward is usually computed by the agent process itself,
rather than given by the environment as the model description implies. However,
in our formal model it is external to the agent, because the agent cannot change
the reward function.

The model so far maps to two the notions we needed: ``\acl{MR}'' is the environment,
``to play'' is to run the process so that the agent chooses actions.
\subsection{Considerations and characteristics of \acp{SDP}\label{subsec:considerations-sdp}}
We may also call an \ac{SDP} a \newconcept{task}, when we are emphasising its nature
as a problem that the agent has to solve.

In this whole work we assume all \aclp{SDP} are \newconcept{fully observable}. That
is, the agent's sensations fully determine which state the environment is in. In
general, that may not be the case. However, the formally defined notions cover
only this case.

\subsubsection{Finite and infinite \acp{SDP}}
An \ac{SDP} is \newconcept{finite} if the set of states $\mathcal{S}$ and the
set of possible actions, $\mathcal{A}$, are finite. Otherwise, it is
\newconcept{infinite}. We will treat only finite \acp{SDP} in this work.

\subsubsection{Episodic and continuing \acp{SDP}}
Sometimes it makes sense to divide a task in non-overlapping continued
interactions between the agent and environment. Such tasks are called
\newconcept{episodic}, and they have one final time step. In contrast,
\newconcept{continuing} tasks never stop, in theory
\citep[Section~3.3]{sutton1998introduction}. The problem we work on, \acl{MR},
is an episodic \ac{SDP}.

\subsubsection{Deterministic and stochastic \acp{SDP}}
We have not yet mentioned how the next state of an \ac{SDP} is determined. In
general, the next state is drawn from a probability distribution over all
possible states $\mathcal{S}$, that depends on the past history of actions,
states and rewards.

Sometimes, the probability distribution has all its weight on a single
state, that is, the next state is a function of the previous history of the
process. Such \acp{SDP} are called \newconcept{deterministic}. When an \ac{SDP} is not
deterministic, it is \newconcept{stochastic}. We work with a deterministic
\ac{SDP}, which is why we are able to use search methods (\ffref{sec:search}).

\subsection{Returns}
What is to play ``well''? The agent's goal is, informally, to maximise the rewards it
gets. In general, we maximise the expected future reward at any time step,
that is, the expected \newconcept{return} at time $t$, $R_t$.
We could simply define $R_t$ as the sum of all rewards until the last time step, $T$:
\begin{equation}
  R_t = r_{t+1} + r_{t+2} + \dots + r_T
  \label{eq:undiscounted-reward}
\end{equation}

However, we may be faced with a continuing task, and the final time step may be
infinity. We could very well be faced with infinite return for each action.
If we want to pick the action with maximal return, and all actions have an
infinite return, we are forced to pick one at random.

Instead, we use a more general notion, that of \newconcept{discounted return}:
\begin{equation}
  R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots = \sum_{k=0}^\infty
\gamma^k r_{t+k+1}
\end{equation}
Where $\gamma$, the \newconcept{discount rate}, is a parameter, $0 \leq \gamma \leq
1$. Observe that, if $\gamma=1$, the return is simply the sum of all rewards, as
in \ffref{eq:undiscounted-reward}. If $\gamma<1$, however, we solve our
infinite return problem: as the time step approaches infinity, the weight its
reward is scaled by approaches zero, and $R_t$ converges.
\citep[Subsection~17.1.1]{russell2009aima}

\subsection{\aclp{MDP}\label{subsec:MDP}}
The \ac{SDP} formalism is not really used in practice. \acfp{MDP}, which are a
restricted case of \acp{SDP}, are used instead.

In general, the next state of an \ac{SDP} may depend on all the past states,
actions and rewards. A \acl{MDP} is a \acl{SDP} that follows the Markov property
(\cite[Section~3.5]{sutton1998introduction};
\cite[Section~17.1]{russell2009aima}), defined as follows.

\begin{equation}
  P \lbrace s_{t+1} = s, r_{t+1} = r | s_t,a_t,r_t,s_{t-1},a_{t-1}, \dots, r_1, s_0, a_0 \rbrace =
  P \lbrace s_{t+1} = s, r_{t+1} = r | s_t,a_t \rbrace
\end{equation}

That is, for all $s\in\mathcal{S}$ and $r\in\mathfrak{R}$, the probability that
in the next step the state is $s$ and the reward is $r$ is the same, whether
conditioned on the whole history of past states, actions and rewards, or
conditioned only on the current state and action. More concisely, the
probability distribution over the next possible states and rewards depends only
on the current state and action.

This property enables us to develop agents that choose an action
based only on the current state: in any \ac{MDP}, this decision is just as good as
considering all past states, actions and rewards.

Additionally, algorithms and additional theory developed on top of \acp{MDP} can
be easily adapted to any \ac{SDP}. Turning an \ac{SDP} into an \ac{MDP} is
trivial: let the state $s'_t$ of the \ac{MDP} be the sequence of current and
previous states, actions and rewards of the SDP, $s_t,a_t,r_t,s_{t-1},\dots,s_n$. If the
\ac{SDP} depends on all of its history $n=0$, otherwise we can take data until
$n=t-m$. Indeed, the latter, excluding rewards, is the approach taken in
\citet{mnih2015human}, \citet{kulkarni2016hierarchical}, and this thesis.

It is also possible for the state to encode an abstracted representation of the
past actions and sensations. A repairer agent includes in its state the size of
the screwdriver it grabbed a few seconds ago, not the sensations, actions and
rewards it had while performing such task.

We usually specify an \ac{MDP} task with the tuple $\langle \mathcal{S}, \mathcal{A}_s,
\mathcal{P}^a_{ss'}, \mathcal{R}^a_{ss'}, \gamma \rangle$:
(based on \citet[Section~3.6]{sutton1998introduction})
\begin{itemize}
\item The set of possible states, $\mathcal{S}$.
\item The available actions for each state, as a function of the state,
  $\mathcal{A}_s$. Some formulations use the set of possible actions
  $\mathcal{A}$ instead.
\item The matrix of transition probabilities from each state to another, given
  an action, $\mathcal{P}^a_{ss'} = P(s_{t+1}=s' | s_t=s, a_t=a)$.
\item The expected reward given a state, an action and the next state,
 $\mathcal{R}^a_{ss'} = E \lbrace r_{t+1} | s_t=s, a_t=a, s_{t+1}=s' \rbrace$
\item The discount factor for calculating returns, $\gamma$
\end{itemize}

Note that the model does not explicitly represent the probability distribution
on rewards, only the expectation.

The $\mathcal{P}^a_{ss'}$ and $\mathcal{R}^a_{ss'}$ matrices are usually
intractably big, and indeed may be infinite if the \ac{MDP} is infinite.

\section{Optimal decision-making in \acp{SDP}}
\subsection{Policies and value functions}
A \newconcept{policy} $\pi$ is a probability distribution for each state
$s\in\mathcal{S}$, over the possible actions to take $a \in \mathcal{A}_s$. It
is represented as a probability associated to each state-action pair: $P =
\pi(s, a)$. We may also write $a = \pi(s)$, if the policy is deterministic:
$\pi(s, a')=1$ if $a'=a$ and $\pi(s, a')=0 if  a' \neq a$.

A \newconcept{value} $V^\pi(s)$ is the expected return for an agent following the
policy $\pi$, that is currently in state $s$. We define it as follows:
\begin{equation}
  V^\pi(s) = E_\pi \lbrace R_t | s_t = s \rbrace =
  E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s \right\}
  \label{eq:definition-value}
\end{equation}

We can also define the \newconcept{action-value function} $Q^\pi(s, a)$ of a policy
$\pi$, which is the expected return from taking action $a$ in state $s$ and
following $\pi$ thereafter.
\begin{equation}
  Q^\pi(s, a) = E_\pi \lbrace R_t | s_t = s, a_t = a \rbrace =
  E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s, a_t = a \right\}
  \label{eq:definition-qvalue}
\end{equation}

\citep[Section~3.7]{sutton1998introduction}

We can calculate the value of a state by doing the average of the
expected values of all the actions, weighted by the probability of each action
being taken. The probability of each action being taken is determined by the
policy, so we get the following identity:
\begin{equation}
  \begin{split}
  \sum_{a\in\mathcal{A}_s}\pi(s, a) Q^\pi(s, a) & = \sum_{a\in\mathcal{A}_s}\pi(s, a) E_\pi \lbrace R_t | s_t = s, a_t = a \rbrace \\
  & = E_\pi \lbrace R_t | s_t = s \rbrace = V^\pi(s)
  \end{split}
  \label{eq:equivalence-qvalue-value}
\end{equation}


\subsection{Optimal policy\label{subsec:optimal-policy}}

Some of the policies will have higher values than others. The policy with the
maximum value for a state is called the \newconcept{optimal policy} for that state,
denoted by $\pi^*_s$. The optimal policy for a state is that which maximises its utility.
\begin{equation}
\pi^*_s = \argmax_\pi V^\pi(s)
\end{equation}
It is also important that the optimal policy is independent of the state it
starts with, if we don't cut the return at a time-step before the final
time-step (that is, the \newconcept{horizon} is infinite) and we use discounted
returns. So, we can just write $\pi^*$ to refer to the optimal policy.

The value of a state when following the optimal policy, $V^{\pi^*}(s)$, is the
\newconcept{true value}, or optimal value, of a state. Thus, we will just write $V(s)$ to refer to it.

If we know the true value function of all the states and the transition model of
the environment, we can calculate the optimal policy:
\begin{equation}
  \pi^*(s) = \argmax_{a\in\mathcal{A}_s} \sum_{s'\in\mathcal{S}}\mathcal{P}^a_{ss'}V(s)
  \label{eq:optimal-policy-value}
\end{equation}

Where ties are broken arbitrarily. Notice that we wrote it as a mapping from
states to actions, and not as a mapping from states and actions to probability
weights. This is because optimal policies are deterministic.

\citep[Subsection~17.1.2]{russell2009aima}

\subsection{Bellman equations}
Both value and action-value functions satisfy a recursive relationship that is
very widely used in reinforcement learning algorithms: the Bellman equations.

Starting with \ffref{eq:definition-value}, we separate the first reward
from the sum of future rewards to obtain the Bellman equation for values:
\begin{equation}
\begin{split}
  V^\pi(s) & = E_\pi \lbrace R_t | s_t = s \rbrace \\
  & = E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s \right\} \\
  & = E_\pi \left\{ \left. r_{t+1} + \gamma\sum_{k=0}^\infty \gamma^kr_{t+k+2} \right| s_t = s \right\} \\
  & = \sum_{a\in\mathcal{A}_s} \left( \pi(s, a) \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
\left[\mathcal{R}^a_{ss'} + \gamma E_\pi \left\{ \left. \sum_{k=0}^\infty
\gamma^kr_{t+k+2} \right| s_{t+1} = s' \right\} \right] \right) \\
  & = \sum_{a\in\mathcal{A}_s} \left( \pi(s, a) \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
\left[\mathcal{R}^a_{ss'} + \gamma V^\pi(s') \right] \right)
\end{split}
\label{eq:bellman-v}
\end{equation}

And let us do the same for action-values, starting with
\ffref{eq:definition-qvalue}:

\begin{equation}
  \begin{split}
    Q^\pi(s, a) & = E_\pi \lbrace R_t | s_t = s, a_t = a \rbrace \\
    & = E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s, a_t = a \right\} \\
    & = E_\pi \left\{ \left. r_{t+1} + \gamma\sum_{k=0}^\infty \gamma^kr_{t+k+2} \right| s_t = s, a_t = a \right\} \\
    & = \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'} \left[\mathcal{R}^a_{ss'} +
      \gamma E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+2} \right| s_{t+1}
        = s' \right\} \right] \\
    & = \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma V^\pi(s') \right]
  \end{split}
\end{equation}

\citep[Section~3.7]{sutton1998introduction}

And if we then substitute in \ffref{eq:equivalence-qvalue-value}:
\begin{equation}
  Q^\pi(s, a) = \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma \sum_{a'\in\mathcal{A}_{s'}}\pi(s', a')Q^\pi(s', a') \right]
    \label{eq:bellman-q}
\end{equation}


\subsubsection{Bellman equations with the optimal policy}
Recall from \ffref{eq:optimal-policy-value} that the optimal policy takes
the action that maximises the true value of the next state. So, let's put this
notion into \ffref{eq:bellman-v}:
\begin{equation}
  \begin{split}
  V(s) &= \sum_{a\in\mathcal{A}_s} \left( \pi^*(s, a)
    \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma V^\pi(s') \right] \right) \\
  &= \max_{a\in\mathcal{A}_s}\sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
\left[\mathcal{R}^a_{ss'} + \gamma V^\pi(s') \right]
  \end{split}
\label{eq:bellman-v-optimal}
\end{equation}
And \ffref{eq:bellman-q}:
\begin{equation}
  \begin{split}
  Q(s, a) &= \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma \sum_{a'\in\mathcal{A}_{s'}}\pi^*(s', a')Q^\pi(s', a') \right] \\
  &= \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma \max_{a'\in\mathcal{A}_{s'}}Q^\pi(s', a') \right]
  \end{split}
\label{eq:bellman-q-optimal}
  \end{equation}
(\cite[Section~17.2.1]{russell2009aima}, \cite[Section~3.8]{sutton1998introduction})

These optimal Bellman equations are the basis of most modern reinforcement
learning algorithms.


\section{\acl{RL}}
Unless stated otherwise, concepts in this section are taken from
\citet{sutton1998introduction}. More concrete citations may also be given.

\acf{RL} is about an agent learning from experience how to behave to maximise
rewards over time. This experience is usually gathered by interacting with the
environment.

\subsection{\acl{VI}\label{subsec:VI}}
\acf{VI} is an algorithm, part of the Dynamic Programming collection of
algorithms for \ac{RL}. Those algorithms can compute optimal policies for an
\ac{MDP}, given a perfect model of the environment ($\mathcal{P}^a_{ss'}$,
$\mathcal{R}^a_{ss'}$, and the parameter $\gamma$).

\ac{VI} works by keeping a table with the values of all states, and
turning the optimal value Bellman equation (\ffref{eq:bellman-v-optimal}) into an
update rule for the table. \ac{VI} is described in \ffref{alg:value-iteration}.

\begin{algorithm}[hbtp]
\begin{algorithmic}
\State Initialise $V(s)$ arbitrarily, for example $V(s)=0 \; \forall s\in\mathcal{S}$
\Repeat
  \State $\Delta \gets 0$, is the maximum update magnitude this iteration
  \For{each $s\in\mathcal{S}$}
    \State $v \gets V(s)$
    \State $V(s) \gets \max_{a\in\mathcal{A}_s}\sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'} \left[\mathcal{R}^a_{ss'} + \gamma V^\pi(s') \right]$
    \State $\Delta \gets \max(\Delta, \left|V(s) - v \right|)$
    \EndFor
\Until{$\Delta < \theta$ a small constant}
\State Output optimal policy using \ffref{eq:optimal-policy-value}
\end{algorithmic}
\caption{\acl{VI} \citep[Section~4.4]{sutton1998introduction}}
\label{alg:value-iteration}
\end{algorithm}

The value table kept in \ac{VI} is guaranteed to converge the true value $V^*$
under the same conditions that guarantee the existence of the latter

\citep[Section~4.4]{sutton1998introduction}

There is another variant of \acl{VI}. In each outer iteration, we update only
one of the values in the table. As long as none of the values stops being
updated at a certain point in the computation, $V(s)$ will still converge to $V^*(s)$.

This does not decrease the amount of computation required to approximate the
optimal value function. However, sweeping over all states is often infeasible,
so this allows the algorithm to start making progress without having to do a
single whole sweep.

We can take advantage of this, and update more often the more promising states,
to be able to terminate \acl{VI} earlier and still have a good enough policy.
\citep[Section~4.5]{sutton1998introduction}

 We could also just update the value of whatever state the agent ended up in
from the previous iteration, provided that we make the agent eventually visit
all states, and still converge to the optimal policy. This one of the basic
ideas in the Sarsa algorithm in \ffref{subsec:sarsa}, Q-learning,
\aclp{DQN} and many others similar in spirit.

\subsection{Exploration-exploitation and \texorpdfstring{$\varepsilon$}{Îµ}-greedy policies}
Agents that are interacting with an environment and learning while collecting
rewards face the \newconcept{exploration-exploitation trade-off}. Should they take the
current maximum return action, or take an action with less return, that may turn
out to have a higher return when the internal value function is closer to the
optimal function?

One way to deal with this trade-off is to follow an
\newconceptspec{$\varepsilon$-greedy policy}{$\varepsilon$-greedy Policy}.
Recall from \ffref{subsec:optimal-policy} that optimal policies,
and ``optimal'' policies based on a sub-optimal value function, always take one
action for any state. Instead of taking only that action, $\varepsilon$-greedy
policies:
\begin{itemize}
\item Take the current deterministic policy's action $\pi(s)$ with probability
$1-\varepsilon$.
\item Take an uniformly randomly sampled $a\in\mathcal{A}_s$ with probability
$\varepsilon$.
\end{itemize}
Where $\varepsilon$ is a parameter, $0\leq\varepsilon\leq 1$. Often,
$\varepsilon=0.1$.

\citep[Section~2.2]{sutton1998introduction}

\subsection{Sarsa\label{subsec:sarsa}}

Suppose an agent knows the optimal value function, $V^*(s)$, and is in state $s$.
How would it go about choosing its next action? Maybe it uses an
$\varepsilon$-greedy optimal policy, as seen in the previous section:

\begin{equation}
  \pi^*(s) = \argmax_{a\in\mathcal{A}_s} \sum_{s'\in\mathcal{S}}\mathcal{P}^a_{ss'}V^*(s)
  \tag{\ref{eq:optimal-policy-value} revisited}
\end{equation}

Note that we need a model of the environment, $\mathcal{P}^a_{ss'}$, as well as
the optimal value function $V^*(s)$. However, if we know the action-value
function, we do not need a model of the environment.
\begin{equation}
  \pi^*(s) = \argmax_{a\in\mathcal{A}_s} Q^*(s, a)
  \label{eq:q-policy}
\end{equation}

For this reason, methods that learn action-value functions are called
\newconcept{model-free methods} \citep[Subsection~21.3.2]{russell2009aima}.

\begin{algorithm}[h]
\caption{Sarsa \citep[Section~6.4]{sutton1998introduction}}
\label{alg:sarsa}
\begin{algorithmic}
\State Initialise $Q(s, a)$ arbitrarily, for example $Q(s, a)=0 \; \forall
s\in\mathcal{S}$
\RepeatComment{for each episode}
  \State Set $s$ to the current, initial, state
  \State Choose action $a$ for $s$, sampling from $a \sim \pi^\varepsilon_Q(s)$
  \RepeatUntilComment{for each step of episode}
    \State Take action $a$, observe reward $r$, state $s'$
    \State Choose action $a'$ for $s'$, sampling from $a' \sim \pi^\varepsilon_Q(s')$
    \State $Q(s, a) \gets (1-\alpha)Q(s,a) +
      \alpha \left[r + \gamma Q(s', a') \right]$ (update step)
    \State $s \gets s'$; $a \gets a'$
  \EndRepeatUntilComment{$s$ is terminal}
\EndRepeatComment
\end{algorithmic}
\end{algorithm}

In \ffref{alg:sarsa} we describe Sarsa, which estimates the
optimal $Q^*$ function under an $\varepsilon$-greedy policy to maximise reward.

$\pi^\varepsilon_Q$ is an $\varepsilon$-greedy policy based on
the deterministic greedy policy based on the current estimate of $Q$, as per
\ffref{eq:q-policy}.

$\alpha$ is the \newconcept{learning rate}, $0\leq\alpha\leq 1$. Because we
don't have the model or policy, only \emph{samples} from them, we cannot
completely update our $Q$ following the Bellman equation. Thus, we instead
\emph{move our value towards} the $Q$-value based on the next one according to
something analogous to the Bellman equation, but we keep $(1-\alpha)$ of the old
value and only account for the new value weighted by $\alpha$.

Sarsa's name comes from the quintuple of values used in its update:
$\langle s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\rangle$.

\citep[Section~6.4]{sutton1998introduction}

\subsubsection{Function approximation}
For interesting problems, it is usually infeasible to store the $Q$ function for
all states and values. Instead, we use a learned function that approximates $Q$.
Desirable approximate functions not only store values close to those of the states and
actions the agent has seen, but also \emph{generalise} to unseen states and
actions. A very desirable method for learning such approximate functions is the
use of \acfp{NN}. \acfp{DQN} \citep{mnih2015human} are a version of Q-Learning
(similar to Sarsa, but using $\max_a Q(s_{t+1},a)$ instead of $Q(s_{t+1},
a_{t+1})$ for updates) that use \acp{NN} for approximating the action-value
function.

When using function approximation in Sarsa, the only step changed is the $Q$
update step. Instead of updating a table with the learning rate, it updates the
function being learned, in a manner that depends on the function.

\subsection{Shaping\label{subsec:shaping}}
Shaping is the practice of giving an agent intermediate rewards that are not
present in the environment. They aim to make learning easier by giving the agent
more frequent feedback. However, rewards added by shaping may change the
behaviour of the agent from what would be the optimal behaviour with the original
\ac{MDP}. Indeed, this happened while
conducting naive learning experiments with shaping for this work
(\ffref{subsec:evaluation-shaping}).

The following definitions and observations are taken from, and proved in,
\citet{ng1999policy}.

Let $\mathcal{M} = \langle \mathcal{S}, \mathcal{A},
\mathcal{P}^a_{ss'}, \mathcal{R}^a_{ss'}, \gamma \rangle$ be the \ac{MDP} the
agent interacts in. We change that \ac{MDP} for another one, $\mathcal{M}' = \langle
\mathcal{S}, \mathcal{A}, \mathcal{P}^a_{ss'}, \mathcal{R}'^a_{ss'}, \gamma
\rangle$, where $\mathcal{R}'^a_{ss'} = \mathcal{R}^a_{ss'} + F(s, a, s')$.
$F : \mathcal{S} \times A \times \mathcal{S} \mapsto \mathbb{R}$ is a bounded
real-valued function called the \newconcept{shaping function}.

A shaping function $F(s, a, s')$ does not alter the optimal policy if and only
if it is a \newconcept{potential-based shaping function}. That is, there exists a
function $\phi(s)$, $\phi : \mathcal{S} \mapsto \mathbb{R}$ such that:
\begin{equation}
  F(s, a, s') = \gamma \phi(s') - \phi(s)
  \label{eq:shaping-reward}
\end{equation}

Potential-based shaping functions are robust: near-optimal policies
in $\mathcal{M}$ remain near-optimal policies in $\mathcal{M'}$: if $\left|
V^\pi_M - V^*_M \right| < \varepsilon$, then $\left| V^\pi_{M'} - V^*_{M'}
\right| < \varepsilon$.

\section{Hierarchical Reinforcement Learning\label{sec:hierarchical-rl}}

Suppose Alice wants to eat a salad. She needs the ingredients, so, she needs to
go to the grocery store. To accomplish that, she needs to get out of the house,
follow the street, \dots To accomplish the first, she needs to get up from the
chair, get out the room, and navigate to the front door. To get up from the
chair, she needs to tense her leg muscles in this way, move her arms in that
way, \dots

Like most if not all humans (and animals), Alice accomplishes tasks by taking
large abstract actions, that are divided into actions, that in turn are divided
into actions, and so on until she reaches contractions of muscle fibres.

Each sub-task can be learned and perfected individually, in all instances it is
performed. For example, learning to walk is useful for going to the grocery
store or going to school, and it gets perfected every time Alice (among other
things) goes to either of the two places.

\subsection{Options}

How may we encode this helpful intuition into reinforcement learning agents?
\citet{sutton1999between} have an answer. The agent may take
\newconcept{options} instead of actions at every state. Options are courses of
action that last one or more time steps, and follow their own policy. Options take
other options and have their own policy, which can be improved every time they
are taken.

An option is a triple $\langle \mathcal{I}, \pi, \beta \rangle$:
\begin{itemize}
  \item $\mathcal{I} \subseteq \mathcal{S}$, the set of states the action can be
    \newconcept{initiated} in.
  \item $\pi(s, a)$, $\pi : \mathcal{S} \times \mathcal{A} \mapsto \mathbb{R}$,
the option's policy.
  \item $\beta(s)$, $\beta : \mathcal{S} \mapsto [0, 1]$, the probability that
the option is interrupted in state $s$.
\end{itemize}

The policy for an option only needs to be defined for a subset $S_o \subseteq
\mathcal{S}$ of the states, as we can define $\beta(s) = 1$ for $s \in S_o$.
Usually also, the action can be initiated wherever its policy is defined, that
is, $\mathcal{I} = S_o$

Normal actions are a special case of options, of duration 1 time step. The
option corresponding to an action $a$ is defined as follows:
\begin{itemize}
  \item $\mathcal{I} = \left\{ s \in \mathcal{S} | a \in \mathcal{A}_s \right\}$
  all the states the action can be taken in.
  \item $\pi(s, a') = 1$ if $a'=a$, otherwise $\pi(s, a') = 0$; for all
$s\in\mathcal{I}$.
  \item $\beta(s) = 1$ for all $s\in\mathcal{S}$.
\end{itemize}

\subsection{Semi-Markov options}
Sometimes it is desirable that actions end after a certain ``timeout'', as well
as in certain states, to avoid agents getting stuck. This case is accommodated
with \newconcept{semi-Markov options}, that depend on the whole history of
states, actions and rewards since they start.

Let a semi-Markov option start in time $t$ and ends in time $\tau$. We call the
sequence $s_t,a_t,r_t,s_{t+1},a_{t+1},\dots,r_\tau,s_\tau$ the
\newconcept{history}, denoted by $h_{t\tau}$. The set of all possible
$h_{t\tau}$ is $\Omega$

An semi-Markov option is a triple $\langle \mathcal{I}, \pi, \beta \rangle$,
with only $\pi$ and $\beta$ differing from the Markov option case:
\begin{itemize}
  \item $\pi(h, a)$, $\pi : \Omega \times \mathcal{A} \mapsto \mathbb{R}$,
the option's policy.
  \item $\beta(h)$, $\beta : \Omega \mapsto [0, 1]$, the probability that
the option is interrupted in state $s$.
\end{itemize}

Options that take other options as actions are semi-Markov, even if all the
underlying options are Markov options.

\subsection{\acfp{SMDP}}

A \ac{SMDP} is defined by:
\begin{itemize}
  \item A set of states $\mathcal{S}$.
  \item A set of actions. We call it $\mathcal{O}$, because this set will be the
    set of possible options in our case. Being a set of options, each has some
    possible initial states. We denote the options available in state $s$ as
$\mathcal{O}_s$.
  \item An expected cumulative discounted reward after taking action $o \in
    \mathcal{O}$ when in state $s \in \mathcal{S}$. We denote it by $r^o_s$. Let
    $t+k$ be the random time at which $o$ terminates. Let $\varepsilon(o, s, t)$
    be the event of option $o$ being initiated in state $s$ at time $t$. Then:
    \begin{equation}
      r^o_s = E\left\{ r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots
      + \gamma^{k-1} r_{t+k} \right\}
    \end{equation}
  \item A well defined joint distribution of next state and transit time, $p(s,
    o, s', k)$, $p : \mathcal{S} \times \mathcal{O} \times \mathcal{S} \times
    \mathbb{N} \mapsto [0, 1]$. For our purposes, we only need $p^o_{ss'}$:
    \begin{equation}
      p^o_{ss'} = \sum_{k=1}^\infty p(s, o, s', k) \gamma^k
    \end{equation}
    This describes the likelihood of reaching a state $s'$ from state $s$ when
    taking option $o$, discounted depending on the time taken to reach it. The
    usefulness of this term will be apparent in the \ac{SMDP}'s Bellman equation
  (\ffref{eq:smdp-bellman-q}).
  \item The discount factor $\gamma$.
\end{itemize}

So we treat it as the tuple $\langle \mathcal{S}, \mathcal{O}, r^o_s, p^o_{ss'},
\gamma\rangle$.

\subsection{Action-value Bellman equation and Sarsa}
The Bellman equation for action-values in \acp{SMDP} ends up looking very
similar to the one for \acp{MDP}.
\begin{equation}
  Q^\pi(s, o) = r^o_s + \sum_{s'}p^o_{ss'} \sum_{o' \in \mathcal{O}_{s'}} \pi(s', o')Q^\pi(s', o')
  \label{eq:smdp-bellman-q}
\end{equation}

The factor $p^o_{ss'}$ is useful because it incorporates both the probability of
reaching a state and the discount its action-value would incur. Thus, it is
exactly what the Q-value of the state we arrive in should be weighted by.

And of course, we take the maximum action-value if we are defining the optimal policy:
\begin{equation}
  Q^\pi(s, o) = r^o_s + \sum_{s'}p^o_{ss'} \max_{o' \in \mathcal{O}_{s'}} Q^*(s', o')
\end{equation}

The Sarsa update looks like this (analogous to the Q-learning update from
\citet[Section~3.2]{sutton1999between}):
\begin{equation}
  Q(s_t, o_t) \gets (1-\alpha)Q(s_t, o_t) + \alpha \left[r_{t:t+k} + \gamma^k Q(s_{t+k}, o_{t+k}) \right]
\end{equation}
Where $s_t$, $o_t$ are the currently selected state and option, $s_{t+k}$ and
$o_{t+k}$ are the next selected state and option, and $k$ is the number of time
steps between $s_t$ and $s_{t+k}$. $r_{t:t+k}$ is the cumulative discounted
reward over the indicated time range.

Note that all these expressions reduce to their ordinary \ac{MDP} counterparts
when the option corresponds to a primitive action.

%DIJOUS
\section{Search in deterministic \acp{MDP}\label{sec:search}}

It can be desirable for the agent to act optimally on the first try, without
having to interact with the environment and learn by trial and error. If the
agent has a perfect model of the environment, this becomes possible.

This is effectively what \acl{VI} does (\ffref{subsec:VI}). However, if the
\ac{MDP} has a deterministic transition model, a much more efficient class of
solutions become possible: \newconcept{search} algorithms.

\subsection{Search problem formulation}

A problem can formally defined by the tuple $\langle \mathcal{S}, s_0,
\mathcal{A}_s, f, \mathcal{S}_G, c \rangle$:

\begin{itemize}
  \item The set of states $\mathcal{S}$.
  \item The actions available in each state $\mathcal{A}_s$.
  \item An initial state $s_0\in\mathcal{S}$.
  \item A deterministic transition function $f(s, a)$, $f : \mathcal{S}
\times \mathcal{A} \mapsto \mathcal{S}$.
  \item A non-empty set of goal states $\mathcal{S}_G \subseteq \mathcal{S}$.
Often defined with a function that tests if a state is in $\mathcal{S}_G$.
  \item A step cost function $c(s, a, s')$, $c : \mathcal{S} \times \mathcal{A}
    \times \mathcal{S} \mapsto \mathbb{R}$.
\end{itemize}

Starting from the initial state $s_0$, the agent must find a
\newconcept{solution}. A solution is a sequence of actions actions $a_1, a_2,
\dots, a_n$ that ``leads to the goal''. Since the transition function is
deterministic, a sequence of actions always brings the agent to the same state.
Thus, the sequence of actions generates the sequence of states $s_1, s_2, \dots
s_n$, where $s_n = f(s_{n-1}, a_n)$. That the sequence ``leads to the goal''
means that $s_n \in \mathcal{S}_G$.

The transition function $f$ can be seen as defining a directed graph: the
possible states are the nodes, and the actions are directed edges. Any possible sequence of
states and actions is a path of this graph, so such sequences are also called
\newconcept{paths}. We can see the step cost function as a weight on each edge
of the graph.

If possible, we want an agent to find an \newconcept{optimal solution}, that is,
one where the path has minimal cost. The cost of a path is the sum of the costs
of all the state transitions taken, that is:
\begin{equation}
C(s_0,\dots,s_n) = \sum_{i=0}^{n-1}c(s_i, a_{i+1}, s_{i+1})
\end{equation}
With this definition of path cost, we can view a search problem as finding a
minimum weight path from $s_0$ to any state in $\mathcal{S}_G$ on the directed
graph. Thus, graph minimum path search algorithms and algorithms for search
problems are roughly the same. Indeed, we can use the well-known Dijkstra
algorithm for finding optimal solutions to search problems.

(\cite[Section~3.1]{russell2009aima})

Note that, since the transition is deterministic, we can write without loss of
information $c(s, a) = c(s, a, f(s, a))$. Also, often the step cost is just
$c(s,a)=1 \; \forall s, a$, so the path cost is the path length. 

\subsubsection{Analogy with \acp{MDP}}
We can draw direct analogies between each element of a search problem and each
element of an \ac{MDP}. Search problems can be seen as a special case of
\acp{MDP}. Reducing a search problem to an \ac{MDP} means that we can create an
\ac{MDP} formulation such that an optimal policy for that \ac{MDP} is also an
optimal solution for the search problem. Reducing an \ac{MDP} to a search
problem is analogous.

For the following discussion, recall the formalisation of \acp{MDP} given in
\ffref{subsec:MDP}. Also remember the types of \ac{SDP} from
\ffref{subsec:considerations-sdp}, which apply to \ac{MDP} as well.

The set of states $\mathcal{S}$ and the actions available in each state
$\mathcal{A}_s$ are directly analogous. The initial state $s_0$ is very clear in
episodic \acp{MDP}, and even in continuing \acp{MDP} the agent has to start
interacting in some state. For the transition model, we can define
$\mathcal{P}^a_{ss'} = 1$ if $s' = f(s, a)$ and otherwise $\mathcal{P}^a_{ss'} =
0$.

In \acp{MDP}, we seek to \emph{maximise} an expected reward function
$\mathcal{R}^a_{ss'}$. In search problems, we seek to \emph{minimise} a cost
function $c(s, a, s')$. We can reduce a search problem to an \ac{MDP} by
defining $R^a_{ss'} = C - c(s, a, s')$, where $C$ is a constant. $C$ can be 0 if
we allow negative rewards or costs, or it can be a number that bounds the cost
function $C \geq c(s, a, s') \forall s,s' \in \mathcal{S}, a \in \mathcal{A}$.
We can reduce a deterministic \ac{MDP} to a search problem by doing the
inverse procedure: $c(s, a, s') = C -R^a_{ss'}$ with $C$ upper-bounding
$R^a_{ss'}$.

We can account for the discount rate $\gamma$ in the \ac{MDP} we are reducing by slightly
modifying the search problem. We can redefine the path cost to be:
\begin{equation}
C(s_0,\dots,s_n) = \sum_{i=0}^{n-1} \gamma^i c(s_i, a_{i+1}, s_{i+1})
\end{equation}

We need only deal with the goal states now, $\mathcal{S}_G$. If we convert a
search problem into an \ac{MDP}, we can make it an episodic \ac{MDP} and
terminate it whenever a goal state would reached. But what if the \ac{MDP} is
continuing?

\subsubsection{The on-line setting\label{subsec:online-setting}}
A solution to a search problem is a path that starts in the initial state and
ends in the goal. Therefore, algorithms that solve search problems \emph{cannot
terminate before reaching a goal state}. Thus, we cannot somehow create a search
problem without a goal and solve it.

We can instead use the \newconcept{on-line setting}
(used in \citet{lipovetzky2015classical}, original source unknown). At each time
step of the would-be continuing \ac{MDP}, a new planning problem is created.
Optimal paths to the goal are searched, but there is no goal. After a set amount
of time, the search algorithm is terminated. The first action of the path with
the least cost (so, the most reward) is taken.

This approach can also be used for episodic \acp{MDP} where the goal may be too
far to be tractable with a certain search algorithm.

\subsection{\acl{BFS}}
\acf{BFS} is one of the basic algorithms for solving search problems.
The algorithm is breadth-first tree traversal, but adapted to graphs
in general. We show it in \ffref{alg:bfs}.

The algorithm uses the following strategy: first it expands the root node, then
each of its successors, then each of the successors' successors, \dots At each
iteration, it expands the shallowest node in the frontier, with ties broken
by order of expansion. To \newconcept{expand} a node is to check if any of
its children is a goal and add them to the frontier. The \newconcept{frontier}
is a data structure, in this case a \ac{FIFO} queue, that keeps the nodes we
will expand in the future.

\ac{BFS} is an instance of an \newconcept{uninformed search} (or
\newconcept{blind search}) algorithm. It has no information about states beyond
what is provided in the problem definition. This is in contrast to
\newconcept{informed} or \newconcept{heuristic} search algorithms, that have
some domain knowledge about which expanded nodes are more promising.

Note that \ac{BFS} always finds a solution (it is \newconcept{complete}), if
there is one and it is not terminated prematurely. It is only an optimal
solution if the path cost is a non-decreasing function of path length, which is
true when all actions have the same cost. Thus, \ac{BFS} is optimal in these
conditions. The space and time complexity of \ac{BFS} are $O(b^d)$, where $b$ is
the \newconcept{branching factor}, or number of possible actions at each node,
and $d$ is the \newconcept{depth} that is explored.

\citep[Sections~3.3,~3.4]{russell2009aima}

\begin{algorithm}[hbtp]
\begin{algorithmic}
  \Function{Solution}{$node$}
    \If{$node.\textsc{Action}=\varnothing$}
      \Return an empty list
    \EndIf
      \State $s \gets \Call{Solution}{node.\textsc{Parent}}$
      \State \Return $\Call{List-Concat}{s, node.\textsc{Action}}$
  \EndFunction
  \Function{Child-Node}{$problem, parent, action$}
    \State \Return a node with:
      \begin{itemize}
        \item[] $\textsc{State} = problem.\Call{Result}{parent.\textsc{State}, action}$,
        \item[] $\textsc{Parent} = parent$, $\textsc{Action}=action$,
        \item[] $\textsc{Path-Cost} = parent.\textsc{Path-Cost} \,+\,
          problem.\Call{Step-Cost}{parent.\textsc{State}, action}$
      \end{itemize}
  \EndFunction
  \Function{Breadth-First-Search}{$problem$}
    \State $node \gets$ a node with:
      \begin{itemize}
        \item[] $\textsc{State} = problem.\textsc{Initial-State}$, $\textsc{Path-Cost}=0$,
        \item[] $\textsc{Parent} = \varnothing$, $\textsc{Action} = \varnothing$
      \end{itemize}
    \If{$problem.\Call{Goal-Test}{node.\textsc{State}}$}
      \Return $\Call{Solution}{node}$
    \EndIf
    \State $frontier \gets$ an empty \ac{FIFO} queue
    \State $frontier \gets \Call{Queue-Insert}{frontier, node}$
    \State $explored \gets$ an empty set
    \Loop
      \If{$\Call{Empty?}{frontier}$}
        \Return failure
      \EndIf
      \State $node \gets \Call{Pop}{frontier}$ \Comment Get the shallowest node
      in $frontier$.
      \State $explored \gets \Call{Set-Insert}{explored, node}$
      \For{\textbf{each} $action$ \textbf{in} $problem.\Call{Actions}{node.\textsc{State}}$}
        \State \Comment Expand each of the node's children.
        \State $child \gets \Call{Child-Node}{problem, node, action}$
        \If{$child.\textsc{State}$ is not in $explored$ or $frontier$}
          \If{$\Call{Goal-Test}{problem, child.\textsc{State}}$}
            \State \Return $\Call{Solution}{child}$
          \EndIf
          \State $frontier \gets \Call{Queue-Insert}{frontier, child}$
        \EndIf
      \EndFor
    \EndLoop
  \EndFunction
\end{algorithmic}
\caption{\acl{BFS} \citep[Sections~3.3,~3.4]{russell2009aima}}
\label{alg:bfs}
\end{algorithm}

\subsection{Planning and \acl{IW}\label{subsec:iterated-width}}
The \ac{BFS} search algorithm from the previous section did not use information
about the structure of the states or the goal. \ac{BFS} only checks if a state
is equal to another and if it is a goal state. We say that the state
representation is \newconcept{atomic}.

\acf{IW}, in contrast, uses a \newconcept{factored} state and goal
representation. This means that the states are represented by vectors of values,
and that goal checking checks conditions on those values. This may give us no
more information than when checking whether an atomic state is a goal, but often
goals in factored state representations check only one or two values. Search
algorithms and problems with factored state representation are called
\newconcept{planning} algorithms and problems.
\citep[Sections~2.4.7,~3.0]{russell2009aima}

\subsubsection{The problem formulation}

\ac{IW} was introduced by \citet{lipovetzky2012width}. For them, a planning
problem is a tuple $\langle F, I, \mathcal{A}, G, f \rangle$:
\begin{itemize}
  \item $F$ is the set of boolean variables of the problem. Each element of $F$
is either true or false in a given state, and a state is represented by the
truth value of each of the variables. It is a representation of the finite set
of states $\mathcal{S}$ of a search problem.
\item $I$ is the set $I \subseteq F$ of variables that are true in the initial
  state. It is thus a representation of the initial state $s_0$.
\item $\mathcal{A}_f$ is the set of available actions in each state, that is,
  each possible combination of variables.
\item $G$ is the set $G \subseteq F$ of variables that are true in the goal
  states of the search problem, $S_G$.
\item The state transition function $f$ is not explicitly stated by them, but
\ac{IW} uses it.
\end{itemize}
Unstated is the cost step function, which is assumed to be $c(s,a,s') = 1$, so
that the cost is always the path length.

\subsubsection{The algorithm\label{subsec:iw-the-algorithm}}

We describe \ac{IW} in \ffref{alg:iterated-width}. \ac{IW} is several successive
runs of $\textsc{IW}(i)$ for $i=1,2,\dots$ until one of the runs returns a solution.
$\textsc{IW}(i)$ is a modified version of \ac{BFS}, the difference is that it
\newconcept{prunes} some states, that is, it avoids putting them in the frontier
after expanding them. $\textsc{IW}(i)$ prunes a node $n$ if its \newconcept{novelty
  measure} is larger than $i$.

The novelty measure of a node is the size of the smallest \newconcept{$i$-tuple}
in it that has not been ``seen'' before in the search. An $i$-tuple is a tuple of
$i$ variables. That a tuple has been ``seen'' before means that, in a previously
searched state (that is, one that is in the $explored$ set), all of the boolean
variables in that tuple have the same value as they do in the current state. For
clarity, we show an example of a state's novelty measure in
\ffref{tab:novelty-example}.

Checking that the novelty measure is not greater than the current maximum width
is carried out in the $\textsc{Check-Novelty}$ function in
\ffref{alg:iterated-width}.

\begin{table}[hbtp]
\begin{center}
\newcommand{\tp}[1]{\langle #1 \rangle}
\begin{tabular}{c|ccc|cl}
State & $f_1$ & $f_2$ & $f_3$ & Novel tuples &  Novelty \\
  \hline
  1 & F & F & F & $\tp{},\tp{f_1},\tp{f_2},\tp{f_3},\tp{f_1,f_2},$ & 0\\
  & & & & $\tp{f_2,f_3},\tp{f_1,f_3},\tp{f_1,f_2,f_3}$ & \\
  2 & F & T & F & $\tp{f_2},\tp{f_1,f_2},\tp{f_2,f_3},\tp{f_1,f_2,f_3}$ & 1 \\
  3 & T & T & F & $\tp{f_1},\tp{f_1,f_2},\tp{f_1,f_3},\tp{f_1,f_2,f_3}$ & 1\\
  4 & T & F & F & $\tp{f_1,f_2},\tp{f_1,f_2,f_3}$ & 2\\
  5 & T & F & T & $\tp{f_3},\tp{f_1,f_3},\tp{f_2,f_3},\tp{f_1,f_2,f_3}$ & 1 \\
  6 & T & F & T & none & $4 = |F|+1$ \\
  7 & F & F & T & $\tp{f_1,f_2,f_3}$ & 3 \\
\end{tabular}

\end{center}
\caption[Novelty measure example]{Example that shows the novelty measure for
each new state, assuming they are expanded in order, from top to bottom. $f_i$
are the problem's variables, $f_i\in F$. In the ``Novel tuples'' column, the
tuples of variables that have values never seen before are shown, and the
corresponding novelty measure of the state is shown in the ``Novelty'' column. }
\label{tab:novelty-example}
\end{table}

\citet{lipovetzky2012width} define the \newconcept{width} $w$ of a planning
problem to be the minimum $i$ such that $\textsc{IW}(i)$ finds an optimal
solution for that problem. \ac{IW} is quite efficient for problems with low
width, since $\textsc{IW}(i)$ has complexity $O(|F|^i)$. Then they find,
experimentally, that $w$ is small ($w \leq 2$) for many planning problems. Thus,
\ac{IW} is often a very efficient planning algorithm without a domain-specific
heuristic.

\begin{algorithm}[hbtp]
\newcommand{\nchoosek}[2]{\left(\begin{array}{c}#1 \\ #2 \end{array}\right)}
\begin{algorithmic}
  \Function{Update-Novelty}{$seen\_tuples, width, state$}
    \ForAll{tuples of variables, $t$, of size $width$}
      \State $seen\_tuples[t, \Call{Value-Of}{t}] \gets true$
    \EndFor
    \State \Return $seen\_tuples$
  \EndFunction
  \Function{Check-Novelty}{$seen\_tuples, width, state$}
  \State \Comment Check if the novelty measure is not greater than $width$.
    \ForAll{tuples of variables, $t$, of size $width$}
      \If{$seen\_tuples[t, \Call{Value-Of}{t}] = false$}
       \Return $true$
      \EndIf
    \EndFor
    \State \Return $false$
  \EndFunction
  \Function{IW}{$problem=\langle F, I, \mathcal{A}, G, f \rangle, i$}
    \State $seen\_tuples \gets$ map table from all the possible
      $width$-tuple--value pairs to a boolean. Takes up $\nchoosek{|F|}{width}\cdot
      2^i$ bits. Initialise to all $false$.
    \State $\Call{Update-Novelty}{seen\_tuples, i, I}$
    \State Perform \ac{BFS} (\ffref{alg:bfs}), with the following modification.
    \begin{itemize}
	  \item[] When inserting a node into the frontier: only do so if the
		  state's novelty is not greater than $i$. That is, when

        $\Call{Check-Novelty}{seen\_tuples, i, node.\textsc{State}} = true$.

        Then, update $seen\_tuples \gets \Call{Update-Novelty}{seen\_tuples, i,
        node.\textsc{State}}$.
    \end{itemize}
    \State \Return the return value of the performed \ac{BFS}.
  \EndFunction
  \Function{Iterated-Width}{$problem=\langle F, I, \mathcal{A}, G, f \rangle$}
    \State $i \gets 1$
    \Repeat
      \State $r \gets \Call{IW}{problem, i}$
      \State $i \gets i+1$
    \Until{$r$ is not failure or $i>|F|$}
    \State \Return $r$
  \EndFunction
\end{algorithmic}
\caption{\acl{IW} \citep{lipovetzky2012width}}
\label{alg:iterated-width}
\end{algorithm}

%DIMECRES
%\section{\aclp{NN}}
%\subsection{Backpropagation}
%\subsection{\aclp{CNN}}
%\subsection{\aclp{DQN}\label{subsec:dqn}}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
