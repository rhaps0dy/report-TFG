\chapter{Theoretical Background}
The aim of this thesis is to produce a computer program that plays \acl{MR}
well. This problem statement suffices for most communication purposes, but does
not give us enough understanding to reason about the problem and find ways to
solve it. We first need to develop a formal definition of all the notions: ``to
play'', ``\acl{MR}'' and ``well''. Fortunately, most of the required work has
already been done, by other authors.

In this chapter we will define a mathematical model for the problem we are
facing. We will also formally define the algorithms we will use to tackle it,
without concerning ourselves with the details of their implementation on our
computing environment.

\section{\aclp{SDP}}
Let us describe the \acf{SDP} model from
(\cite[Section~3.1]{sutton1998introduction}). There is an \emph{agent} that, every
\emph{time step}, takes an \emph{action} in the \emph{environment}. The
environment is a process that has a \emph{state}. When the agent takes an
action, the environment's state changes. The agent also receives a \emph{reward}
when it takes an action.

More formally. There is a series of discrete time steps, $t=0,1,2,3,\dots$, in
which the agent and the environment interact. At time step $t$, environment is
in state $s_t \in \mathcal{S}$. $\mathcal{S}$ is the finite, but usually very
large, set of possible states. The agent takes an action $a_t$ from the set of
possible actions in the state, $a_t \in \mathcal{A}(s_t)$. In the next time
step, the agent receives a numerical reward $r_{t+1} \in \mathfrak{R}$, and the
environment transitions to a new state $s_{t+1} \in \mathcal{S}$.

The environment defines the set of possible states $\mathcal{S}$, the possible
actions in each state $\mathcal{A}(s)$, the reward for each state $\mathcal{S}$
and the rules for transitioning to the new state in each time step. The agent
simply chooses the action $a_t$ in each time step. The interaction between agent
and environment is illustrated in figure \ref{fig:agent-environment}.

\inkscapefig{agent-environment}{Diagram of interaction between agent and
  environment (\cite[Section~3.1]{sutton1998introduction})}

This model is really flexible. It does not constrain time steps to have the same
length, so each can represent a decision branching point and not actual time. An
example of this can be observed in go, chess, poker and others, where each move
takes a different amount of wall clock time. It is also not constrained how a new
state is chosen after an action: it may depend on anything, maybe even be
stochastic and not deterministic.

The model also accepts different abstraction levels of states and actions: in a
video game, they can be raw pixel data and controller input, or entity position
representation and moving to a certain screen. This idea is the basis of
hierarchical reinforcement learning, which is explained in section
\ref{section:hierarchical-rl}.

It is important to understand that the agent is only the \emph{process} that
\emph{decides} actions, not a physical object or entity. In the case of a robot, the
agent is only the controlling program: the actuators, mechanisms and sensors are part
of the environment. In the case of a video game, the code that emulates
the world and accepts controls is the environment, and the code that trains a
model or plans actions is the agent. This is the case even if the actions to be
taken are high-level, and not settings of force or torque on actuators or
muscles.

Observe also that the reward is usually computed by the agent process itself,
rather than given by the environment as the model description implies. However,
in our formal model it is external to the agent, because the agent cannot change
the reward function.

The model so far maps to two the notions we needed: ``\acl{MR}'' is the environment,
``to play'' is to run the process so that the agent chooses actions.
\subsection{Considerations and characteristics of \acp{SDP}}
We may also call an \ac{SDP} a \emph{task}, when we are emphasising its nature
as a problem that the agent has to solve.

In this whole work we assume all \aclp{SDP} are \emph{fully observable}. That
is, the agent's sensations fully determine which state the environment is in. In
general, that may not be the case. However, the formally defined notions cover
only this case.

\subsubsection{Finite and infinite \acp{SDP}}
An \ac{SDP} is \emph{finite} if the set of states $\mathcal{S}$ and the set of possible
values of the action function, $\lbrace \mathcal{A}(s) | s \in \mathcal{S}
\rbrace$, are finite. Otherwise, it is \emph{infinite}. We will treat only
finite \acp{SDP} in this work.

\subsubsection{Episodic and continuing \acp{SDP}}
Sometimes it makes sense to divide a task in non-overlapping continued
interactions between the agent and environment. Such tasks are called
\emph{episodic}, and they have one final time step. In contrast,
\emph{continuing} tasks never stop, in theory.
(\cite[Section~3.3]{sutton1998introduction})

\subsubsection{Deterministic and stochastic \acp{SDP}}
We have not yet mentioned how the next state of an \ac{SDP} is determined. In
general, the next state is drawn from a probability distribution over all
possible states $\mathcal{S}$, that depends on the past history of actions,
states and rewards.

Sometimes, the probability distribution has all its weight on a single
state, that is, the next state is a function of the previous history of the
process. Such \acp{SDP} are called \emph{deterministic}. When a \ac{SDP} is not
deterministic, it is \emph{stochastic}.

\subsection{Returns}
What is to play ``well''? The agent's goal is, informally, to maximise the rewards it
gets. In general, we maximise the expected future reward at any time step,
that is, the expected \emph{return} at time $t$, $R_t$.
We could simply define $R_t$ as the sum of all rewards until the last time step, $T$:
\begin{equation}
  R_t = r_{t+1} + r_{t+2} + \dots + r_T
  \label{eq:undiscounted-reward}
\end{equation}

However, we may be faced with a continuing task, and the final time step may be
infinity. We could very well be faced with infinite return for each action.
If we want to pick the action with maximal return, and all actions have an
infinite return, we are forced to pick one at random.

Instead, we use a more general notion, that of \emph{discounted return}:
\begin{equation}
  R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots = \sum_{k=0}^\infty
\gamma^k r_{t+k+1}
\end{equation}
Where $\gamma$, the \emph{discount rate}, is a parameter, $0 \leq \gamma \leq
1$. Observe that, if $\gamma=1$, the return is simply the sum of all rewards, as
in equation \ref{eq:undiscounted-reward}. If $\gamma<1$, however, we solve our
infinite return problem: as the time step approaches infinity, the weight its
reward is scaled by approaches zero, and $R_t$ converges.
(\cite[Subsection~17.1.1]{russell2009aima})

\subsection{\aclp{MDP}}
The \ac{SDP} formalism is not really used in practice. \acfp{MDP}, which are a
restricted case of \acp{SDP}, are used instead.

In general, the next state of an \ac{SDP} may depend on all the past states,
actions and rewards. A \acl{MDP} is a \acl{SDP} that follows the Markov property
(\cite[Section~3.5]{sutton1998introduction},
\cite[Section~17.1]{russell2009aima}), defined as follows.

\begin{equation}
  P \lbrace s_{t+1} = s, r_{t+1} = r | s_t,a_t,r_t,s_{t-1},a_{t-1}, \dots, r_1, s_0, a_0 \rbrace =
  P \lbrace s_{t+1} = s, r_{t+1} = r | s_t,a_t \rbrace
\end{equation}

That is, for all $s\in\mathcal{S}$ and $r\in\mathfrak{R}$, the probability that
in the next step the state is $s$ and the reward is $r$ is the same, whether
conditioned on the whole history of past states, actions and rewards, or
conditioned only on the current state and action. More concisely, the
probability distribution over the next possible states and rewards depends only
on the current state and action.

This property enables us to develop agents that choose an action
based only on the current state: in any \ac{MDP}, this decision is just as good as
considering all past states, actions and rewards.

Additionally, algorithms and additional theory developed on top of \acp{MDP} can
be easily adapted to any \ac{SDP}. Turning an \ac{SDP} into an \ac{MDP} is
trivial: let the state $s'_t$ of the \ac{MDP} be the sequence of current and
previous states, actions and rewards of the SDP, $s_t,a_t,r_t,s_{t-1},\dots,s_n$. If the
\ac{SDP} depends on all of its history $n=0$, otherwise we can take data until
$n=t-m$. Indeed, the latter, excluding rewards, is the approach taken in
\cite{mnih2015human}, \cite{kulkarni2016hierarchical} and this thesis.

It is also possible for the state to encode an abstracted representation of the
past actions and sensations. A repairer agent includes in its state the size of
the screwdriver it grabbed a few seconds ago, not the sensations, actions and
rewards it had while performing such task.

We usually specify an \ac{MDP} task with the tuple $\langle \mathcal{S}, \mathcal{A},
\mathcal{P}^a_{ss'}, \mathcal{R}^a_{ss'}, \gamma \rangle$:
(\cite[Section~3.6]{sutton1998introduction}, \cite[Section~2.3]{blanco2015tfg})
\begin{itemize}
\item The set of possible states, $\mathcal{S}$.
\item The available actions for each state, as a function of the state, $\mathcal{A}(s)$.
\item The matrix of transition probabilities from each state to another, given
  an action, $\mathcal{P}^a_{ss'} = P(s_{t+1}=s' | s_t=s, a_t=a)$.
\item The expected reward given a state, an action and the next state,
 $\mathcal{R}^a_{ss'} = E \lbrace r_{t+1} | s_t=s, a_t=a, s_{t+1}=s' \rbrace$
\item The discount factor for calculating returns, $\gamma$
\end{itemize}

We lose the information of the probability distribution of rewards, but that
knowledge is not used in any well-known algorithms.

The $\mathcal{P}^a_{ss'}$ and $\mathcal{R}^a_{ss'}$ matrices are usually
intractably big, and indeed may be infinite if the \ac{MDP} is infinite.

\section{Optimal decision-making in SDP}
\subsection{Policies and value functions}
A \emph{policy} $\pi$ is a probability distribution for each state
$s\in\mathcal{S}$, over the possible actions to take $a \in \mathcal{A}(s)$. It
is represented as a probability associated to each state-action pair: $P =
\pi(s, a)$. We may also write $a = \pi(s)$, if the policy is deterministic:
$\pi(s, a')=1$ if $a'=a$ and $\pi(s, a')=0 if  a' \neq a$.

A \emph{value} $V^\pi(s)$ is the expected return for an agent following the
policy $\pi$, that is currently in state $s$. We define it as follows:
\begin{equation}
  V^\pi(s) = E_\pi \lbrace R_t | s_t = s \rbrace =
  E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s \right\}
  \label{eq:definition-value}
\end{equation}

We can also define the \emph{action-value function} $Q^\pi(s, a)$ of a policy
$\pi$, which is the expected return from taking action $a$ in state $s$ and
following $\pi$ thereafter.
\begin{equation}
  Q^\pi(s, a) = E_\pi \lbrace R_t | s_t = s, a_t = a \rbrace =
  E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s, a_t = a \right\}
  \label{eq:definition-qvalue}
\end{equation}

(\cite[Section~3.7]{sutton1998introduction})

We can calculate the value of a state by doing the average of the
expected values of all the actions, weighted by the probability of each action
being taken.The probability of each action being taken is determined by the
policy, so we get the following identity:
\begin{equation}
  \begin{split}
  \sum_{a\in\mathcal{A}(s)}\pi(s, a) Q^\pi(s, a) & = \sum_{a\in\mathcal{A}(s)}\pi(s, a) E_\pi \lbrace R_t | s_t = s, a_t = a \rbrace \\
  & = E_\pi \lbrace R_t | s_t = s \rbrace = V^\pi(s)
  \end{split}
  \label{eq:equivalence-qvalue-value}
\end{equation}


\subsection{Optimal policy}

Some of the policies will have higher values than others. The policy with the
maximum value for a state is called the \emph{optimal policy} for that state,
denoted by $\pi^*_s$. The optimal policy for a state is that which maximises its utility.
\begin{equation}
\pi^*_s = \argmax_\pi V^\pi(s)
\end{equation}
It is also important that the optimal policy is independent of the state it
starts with, if we don't cut the return at a time-step before the final
time-step (that is, the \emph{horizon} is infinite) and we use discounted
returns. So, we can just write $\pi^*$ to refer to the optimal policy.

The value of a state when following the optimal policy, $V^{\pi^*}(s)$, is the
\emph{true value}, or optimal value, of a state. Thus, we will just write $V(s)$ to refer to it.

If we know the true value function of all the states, we can calculate the optimal policy:
\begin{equation}
  \pi^*(s) = \argmax_{a\in\mathcal{A}(s)} \sum_{s'\in\mathcal{S}}\mathcal{P}^a_{ss'}V(s)
  \label{eq:optimal-policy-value}
\end{equation}

Notice that we wrote it as an action, function of only the state, and not as a
probability, function of both the state and the action. This is because optimal
policies take only one action. Or several, with equal probability, if they all
have the same, maximal, utility.

(\cite[Subsection~17.1.2]{russell2009aima})

\subsection{Bellman equations}
Both value and action-value functions satisfy a recursive relationship that is
very widely used in reinforcement learning algorithms: the Bellman equations.

Starting with equation \ref{eq:definition-value}, we separate the first reward
from the sum of future rewards to obtain the Bellman equation for values:
\begin{equation}
\begin{split}
  V^\pi(s) & = E_\pi \lbrace R_t | s_t = s \rbrace \\
  & = E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s \right\} \\
  & = E_\pi \left\{ \left. r_{t+1} + \gamma\sum_{k=0}^\infty \gamma^kr_{t+k+2} \right| s_t = s \right\} \\
  & = \sum_{a\in\mathcal{A}(s)} \left( \pi(s, a) \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
\left[\mathcal{R}^a_{ss'} + \gamma E_\pi \left\{ \left. \sum_{k=0}^\infty
\gamma^kr_{t+k+2} \right| s_{t+1} = s' \right\} \right] \right) \\
  & = \sum_{a\in\mathcal{A}(s)} \left( \pi(s, a) \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
\left[\mathcal{R}^a_{ss'} + \gamma V^\pi(s') \right] \right)
\end{split}
\label{eq:bellman-v}
\end{equation}

And let us do the same for action-values, starting with equation
\ref{eq:definition-qvalue}:

\begin{equation}
  \begin{split}
    Q^\pi(s, a) & = E_\pi \lbrace R_t | s_t = s, a_t = a \rbrace \\
    & = E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s, a_t = a \right\} \\
    & = E_\pi \left\{ \left. r_{t+1} + \gamma\sum_{k=0}^\infty \gamma^kr_{t+k+2} \right| s_t = s, a_t = a \right\} \\
    & = \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'} \left[\mathcal{R}^a_{ss'} +
      \gamma E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+2} \right| s_{t+1}
        = s' \right\} \right] \\
    & = \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma V^\pi(s') \right]
  \end{split}
\end{equation}

(\cite[Section~3.7]{sutton1998introduction})

And if we then substitute in equation \ref{eq:equivalence-qvalue-value}:
\begin{equation}
  Q^\pi(s, a) = \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma \sum_{a'\in\mathcal{A}(s')}\pi(s', a')Q^\pi(s', a') \right]
    \label{eq:bellman-q}
\end{equation}


\subsubsection{Bellman equations with the optimal policy}
Recall from equation \ref{eq:optimal-policy-value} that the optimal policy takes
the action that maximises the true value of the next state. So, let's put this
notion into equation \ref{eq:bellman-v}:
\begin{equation}
  \begin{split}
  V(s) &= \sum_{a\in\mathcal{A}(s)} \left( \pi^*(s, a)
    \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma V^\pi(s') \right] \right) \\
  &= \max_{a\in\mathcal{A}(s)}\sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
\left[\mathcal{R}^a_{ss'} + \gamma V^\pi(s') \right]
  \end{split}
\end{equation}
And equation \ref{eq:bellman-q}:
\begin{equation}
  \begin{split}
  Q(s, a) &= \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma \sum_{a'\in\mathcal{A}(s')}\pi^*(s', a')Q^\pi(s', a') \right] \\
  &= \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}
    \left[\mathcal{R}^a_{ss'} + \gamma \max_{a'\in\mathcal{A}(s')}Q^\pi(s', a') \right]
  \end{split}
  \end{equation}
(\cite[Section~17.2.1]{russell2009aima})

These optimal Bellman equations are the basis of most modern reinforcement
learning algorithms.


\section{Reinforcement Learning}
And that's why we learn Q-values: to not need to learn a policy too!

\subsection{Policy iteration}
\subsection{Q-learning ?}
\subsection{Sarsa}
\subsection{Hierarchical Reinforcement Learning\label{section:hierarchical-rl}}
Theory about MAXQ and its application to the task we decomposed
s
\section{Planning}

Get ideas from \cite{russell2009aima}.

\subsection{Blind Planning}
\subsection{Iterated Width}

\section{Neural Networks}
\subsection{Backpropagation}
\subsection{Convolutional NNs}

\section{Planning+learning}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../report"
%%% End: 