\newpage
\thispagestyle{empty}
\vfill
\centeredtitle{Abstract}
Traditionally, methods for solving \acp{SDP} have not
worked well with those that feature sparse feedback. Both planning and
reinforcement learning, methods for solving \acp{SDP}, have trouble with it.

With the rise to prominence of the \ac{ALE} in the
broader research community of sequential decision processes, one \ac{SDP} featuring
sparse feedback has become familiar: the Atari game \acl{MR}. In this
particular game, the great amount of knowledge the human player already
possesses, and uses to find rewards, cannot be bridged by blindly exploring in a
realistic time.

We apply planning and reinforcement learning approaches, combined with domain
knowledge, to enable an agent to obtain better scores in this game.

We hope that these domain-specific algorithms can inspire better approaches to
solve \acp{SDP} with sparse feedback in general.
\vfill